\input{preamble}

\title{Solutions to Problem Set 7}
\author{Lei Zhao}

% \usepackage{xcolor}
% \definecolor{linkcolor}
\hypersetup{allcolors=[rgb]{0,0.2,0.6},linkcolor=black}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\hyphenpenalty=687

\begin{document}
\maketitle

This is my own solutions to the problem set 7 of
\href{https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/unit-iii/lecture-15/}{6.041\textsc{sc}}.

\begin{enumerate}
  \pdfbookmark{1}{1}
  \item Consider a sequence of mutually independent, identically
  distributed, probabilistic trials.  Any particular trial results in
  either a success (with probability \(p\)) or a failure.
  \begin{enumerate}
  \item \subpdfbookmark{a}{1.a} Obtain a simple expression for the
    probability that the \(i\)th success occurs before the \(j\)th
    failure.  You may leave your answer in the form of a summation.

    Consider the situation that the \(i\)th success happend at the
    \(k\)th trial.  If \(k \ge i+j\), then there is already at least
    \(j\) failures before the \(i\)th success.  If \(k < i+j\), then
    \(j\)th failture must occur after \(i\)th success.  Thus, the
    probability that the \(i\)th success occurs before the \(j\)th
    failure is the sum of all probabilities of \(i\)th success at the
    \(k\)th trial where \(k < i+j\), which is

    \[\sum_{k<i+j} P(i\text{th success at the }k\text{th trial}) = \sum_{k=i}^{i+j-1} \binom{k-1}{i-1} \, p^i (1-p)^{k-i}.\]

  \item \currentpdfbookmark{b}{1.b} Determine the expected value and variance
    of the number of successes which occur before the \(j\)th failure.

    Let \(X\) be the number of trials until the \(j\)th failure.  Then
    \(X\) can be decompose into a sequence of i.i.d.\ geometric random
    variables \(X_1\), \(X_2\), \dots, and \(X_j\) with parameter
    \(1-p\).  Let \(Y\) be the number of successes before the \(j\)th
    failture.  It can be decomposed into a sequence of i.i.d.\ random
    variables \(Y_1\), \dots, and \(Y_j\), where each
    \(Y_i = X_i - 1\).  Thus, we have
    \begin{align*}
      \E[Y] &= \E[\sum_{i=1}^j Y_i] \\
            &= \sum_{i=1}^j \E[X_i - 1] \\
            &= j(\E[X_1] - 1) \\
            &= j \paren{\frac1{1-p} - 1} \\
            &= \frac{jp}{1-p}, \\
      \intertext{and}
      \var(Y)
            &= \var(\sum_{i=1}^j Y_i) \\
            &= \sum_{i=1}^j \var(X_i -1) \\
            &= j \var(X_1) \\
            &= \frac{jp}{(1-p)^2}.
    \end{align*}

  \item \currentpdfbookmark{c}{1.c} Let \(L_{17}\) be described by a Pascal \textsc{pmf} of order
    \(17\).  Find the numerical values of \(a\) and \(b\) in the following
    equation:
    \[\sum_{l=42}^\infty p_{L_{17}}(l) = \sum_{x=0}^a \binom{b}{x} \, p^x (1-p)^{b-x}.\]
    Explain your work.

    The \textsc{lhs} can be interpreted as the probability that the
    \(17\)th success happens after \(41\)th trial.  That is equal to
    the probability of \(x\) successes in the first \(41\) trials for
    all \(x < 17\), which is
    \[\sum_{x < 17} P(x\text{ successes in the first }41\text{trials}) = \sum_{x=0}^{16} \binom{41}{x} \, p^x (1-p)^{41-x}.\]
    It is easy to see this has the same form of the \textsc{rhs} of
    the original formula.  Thus, \(a = 16\) and \(b = 41\).
  \end{enumerate}

\item \pdfbookmark{2}{2} Fred is giving out samples of dog food.  He makes calls door to
  door, but he leaves a sample (one can) only on those calls for which
  the door is answered \emph{and} a dog is in residence.  On any call
  the probability of the door being answered is \(3/4\), and the
  probability that any household has a dog is \(2/3\).  Assume that
  the events “Door answered” and “A dog lives here” are independent
  and also that the outcomes of all calls are independent.
  \begin{enumerate}
  \item \subpdfbookmark{a}{2.a} Determine the probability that Fred
    gives away his first sample on his third call.

    The probability that Fred gives out a sample on any call is
    \(3/4 \times 2/3 = 1/2\).  Let \(X_k\) be the number of the call
    on which Fred gives away his \(k\)th sample.  Thus, the
    probability to be determined is
    \[p_{X_1}(3) = \paren{\frac12}^2 \frac12 = \frac18.\]

  \item \currentpdfbookmark{b}{2.b} Given that he has given away exactly
    four samples on his first eight calls, determine the conditional
    probability that Fred will give away his fifth sample on his
    eleventh call.

    Due to the memorylessness property of Bernoulli process, the
    probability that Fred will give away his fifth sample on his
    eleventh call, conditioned upon that he has given away exactly
    four samples on his first eight calls, is the same as the
    unconditional probability that he will give away his first sample
    on the third call.  This is the same probability calculated in
    (a), which is \(1/8\).

  \item \currentpdfbookmark{c}{2.c} Determine the probability that he
    gives away his second sample on his fifth call.

    \[p_{X_2}(5) = \binom{4}{1} \paren{\frac12}^5 \paren{\frac12}^3 = \frac{1}{64}.\]

  \item \currentpdfbookmark{d}{2.d} Given that he did not give away
    his second sample on his second call, determine the conditional
    probability that he will leave his second sample on his fifth
    call.

    \begin{align*}
      P(X_2 = 5 \mid X_2 > 2) &= \frac{P(X_2 = 5, X_2 > 2)}{P(X_2 > 2)} \\
                              &= \frac{P(X_2 = 5)}{1 - P(X_2 = 2)} \\
                              &= \frac{4 \cdot 1/2^5}{1 - (1/2)^2} \\
                              &= \frac{1}{6}.
    \end{align*}

  \item \currentpdfbookmark{e}{2.e} We will say that Fred “needs a new
    supply” immediately \emph{after} the call on which he gives away
    his last can.  If he starts out with two cans, determine the
    probability that he completes at least five calls before he needs
    a new supply.

    \begin{align*}
      P(X_2 \ge 5) &= \sum_{i=5}^\infty \binom{i-1}{1} \paren{\frac12}^2 \paren{\frac12}^{i-2} \\
                   &= \sum_{j=0}^1 \binom4j \paren{\frac12}^j \paren{\frac12}^{4-j} \\
                   &= (1 + 4) \frac1{2^4} = \frac{5}{16}.
    \end{align*}

  \item \currentpdfbookmark{f}{2.f} If he starts out with exactly
    \(m\) cans, determine the expected value and variance of \(D_m\),
    the number of homes with dogs which he passes up (because of no
    answer) before he needs a new supply.

    Since the events “Door answered” and “A dog lives here” are
    independent and the outcomes of all calls are independent, we can
    restrict our attention to only the homes with dogs.  Given a home
    with dogs, the probability of the door being answered is \(3/4\);
    and this is the same as a Bernoulli process with parameter
    \(3/4\).  We can decompose \(D_m\) into a sequence of random
    variables \(G_1-1\), \(G_2-1\), \dots, \(G_m-1\), where each
    \(G_i\) is an i.i.d.\ geometric random variable.  Therefore,
    \begin{align*}
      \E[D_m] &= \E[\sum_{i=1}^m (G_i-1)] \\
              &= \sum_{i=1}^m \E[G_1 -1] \\
              &= m (\E[G_1] - 1) \\
              &= \frac{m(1-p)}{p}, \\
      \intertext{and}
      \var(D_m)
              &= \var(\sum_{i=1}^m (G_i-1)) \\
              &= \sum_{i=1}^m \var(G_1) \\
              &= \frac{m(1-p)}{p^2}.
    \end{align*}
  \end{enumerate}

\item \pdfbookmark{3}{3} Let \(T_1\) and \(T_2\) be exponential random variables with
  parameter \(λ\), and let \(S\) be an exponential random variable
  with parameter \(μ\).  We assume that all three of these random
  variables are independent.  Derive an expression for the expected
  value of \(\min\{T_1+T_2, S\}\). \emph{Hint}: See Problem 6.19 in the text.

  Since
  \begin{align*}
    P(\min\{T_1+T_2, S\} > t) &= P(T_1 + T_2 > t) \, P(S > t) \\
                              &= (e^{-λt} + λte^{-λt}) e^{-μt} \\
                              &= (1+λt) e^{-(λ+μ)t}, \\
  \intertext{therefore}
    P(\min\{T_1+T_2, S\} \le t)
                              &= 1 - P(\min\{T_1+T_2, S\} > t) \\
                              &= 1 - (1+λt) e^{-(λ+μ)t}. \\
  \intertext{Take the derivative to get the \textsc{pdf} of \(\min\{T_1+T_2, S\}\),}
    f_{\min\{T_1+T_2, S\}}(t) &= \ddt (1 - (1+λt) e^{-(λ+μ)t}) \\
                              &= -λe^{-(λ+μ)t} + (1+λt)(λ+μ) e^{-(λ+μ)t} \\
                              &= (μ+λ(λ+μ)t) e^{-(λ+μ)t}. \\
    \intertext{Integrate to find the expected value of \(\min\{T_1+T_2, S\}\),}
    \E[\min\{T_1+T_2, S\}]
                              &= \int_0^{+\infty} t \, f_{\min\{T_1+T_2, S\}}(t) \dt \\
                              &= \int_0^{+\infty} (μt+λ(λ+μ)t^2) e^{-(λ+μ)t} \dt \\
                              &= \frac{μ}{λ+μ} \int_0^{+\infty} t \, (λ+μ)e^{-(λ+μ)t} \dt \\
                              &\phantom{{}={}} + λ \int_0^{+∞} t^2 (λ+μ)e^{-(λ+μ)t} \dt \\
                              &= \frac{μ}{(λ+μ)^2} + λ \paren{\frac{1}{(λ+μ)^2} + \paren{\frac{1}{λ+μ}}^2} \\
                              &= \frac{μ+2λ}{(λ+μ)^2}.
  \end{align*}

\item \pdfbookmark{4}{4} A single dot is placed on a very long length of yarn at the
  textile mill.  The yarn is then cut into lengths requested by
  different customers.  The lengths are independent of each other, but
  all distributed according to the \textsc{pdf} \(f_L(ℓ)\).  Let \(R\)
  be be the length of yarn purchased by that customer whose purchase
  included the dot.  Determine the expected value of \(R\) in the
  following cases:
  \begin{enumerate}
  \item \subpdfbookmark{a}{4.a} \(f_L(ℓ) = λe^{−λℓ}\) for \(ℓ \ge 0\).

    This problem is equivalent to the random incidence paradox.  For
    this specific \textsc{pdf} \(f_L\), the resulting \(R\) is an
    Erlang distribution of order \(2\).  Thus,
    \[\E[R] = \frac2λ.\]

  \item \currentpdfbookmark{b}{4.b}
    \(f_L(ℓ) = \frac{λ^3ℓ^2}{2} e^{−λℓ}\) for \(ℓ \ge 0\).

    In this case, \(L\) has an Erlang distribution of order \(3\).  It
    follows that \(R\) has an Erlang distribution of order \(4\).  Thus,
    \[\E[R] = \frac4λ.\]

  \item \currentpdfbookmark{c}{4.c} \(f_L(ℓ) = ℓe^ℓ\), for \(0 \le ℓ \le 1\).

    Section
    \href{https://web.mit.edu/urban_or_book/www/book/chapter2/2.13.html}{2.13}
    \emph{Random Incidence} from \emph{Urban Operations Research} by
    Larson and Odoni is a very good source for this type of problem.
    Another source is section
    \href{https://www.google.com/books/edition/Fundamentals_of_Stochastic_Networks/mTywnGmFsdMC?hl=en&gbpv=1&dq=random+incidence&pg=PT46&printsec=frontcover}{2.8.3}
    \emph{Random Incidence and Residual Time} from \emph{Fundamentals
      of Stochastic Networks} by Oliver C. Ibe.

    Both sources uses an argument by saying the \(f_R(ℓ)δ\) is
    proportional to both \(ℓ\) and \(f_L(ℓ)δ\), that is
    \(f_R(ℓ)δ ∝ ℓ \, f_L(ℓ)δ\).  Since \(f_R(ℓ)\) is a \textsc{pdf} and
    it must integrate to one, then the normalizing constant must be
    \(1/\!\E[L]\).  In this way, we derive that
    \[f_R(ℓ) = \frac{ℓ \, f_L(ℓ)}{\E[L]}.\]
    Then it is obvious that
    \begin{align*}
      \E[L] &= \int_0^1 ℓ^2 e^ℓ \diff ℓ \\
            &= ℓ^2 e^ℓ \Big\vert_0^1 - 2 \int_0^1 ℓ e^ℓ \diff ℓ \\
            &= e - 2 (ℓ e^ℓ - e^ℓ) \Big\vert_0^1 \\
            &= e - 2, \\
      \intertext{and}
      \E[R] &= \int_0^1 \frac{ℓ^3 e^ℓ}{\E[L]} \diff ℓ \\
            &= \frac{ℓ^3 e^ℓ \Big\vert_0^1 - 3\E[L]}{\E[L]} \\
            &= \frac{2(3-e)}{e-2}.
    \end{align*}

    This proportional argument, however, was somewhat elusive to me
    and I spent one whole day ruminating about this argument.
    Fortunately, I eventually found a different way to derive \(f_R\)
    that is \emph{intuitive} to understand.  First, we can consider
    the situation where \(L\) is a discrete r.v.\ and treat the
    continuous version as the limiting case of the discrete version.
    % If \(L\) is discrete, we have a \textsc{pmf} \(p_L\) instead of a
    % \textsc{pdf} \(f_L\) for \(L\).  Let us envision an extreme case
    % where there is no randomness and everything is deterministic.
    %
    % \dots
  \end{enumerate}

\item \pdfbookmark{5}{5} Consider a Poisson process of rate
  \(λ\).  Let random variable \(N\) be the number of arrivals in
  \((0, t]\) and \(M\) be the number of arrivals in \((0, t + s]\),
  where \(t, s ≥ 0\).
  \begin{enumerate}
  \item \subpdfbookmark{a}{5.a} Find the conditional \textsc{pmf} of
    \(M\) given \(N\), \(p_{M \mid N}(m \mid n)\), for \(m \ge n\).

    Due to the memorylessness property of Poisson process, we have
    \[
      p_{M \mid N}(m \mid n) = p_{N_s}(m-n) = \frac{(λs)^{m-n}}{(m-n)!} e^{-λs}.
    \]

  \item \currentpdfbookmark{b}{5.b} Find the joint \textsc{pmf} of
    \(N\) and \(M\), \(p_{N,M}(n, m)\).

    By multiplication rule, we have
    \[
      p_{N,M}(n, m) = p_{N}(n) \, p_{M \mid N}(m \mid n) =
      \begin{cases}
        \dfrac{(λs)^m(t/s)^n}{n!(m-n)!} e^{-λ(t+s)}, & 0 \le n \le m, \\[1em]
        0, & \text{otherwise.}
      \end{cases}
    \]

  \item \currentpdfbookmark{c}{5.c} Find the conditional \textsc{pmf} of \(N\) given \(M\), \(p_{N \mid M}(n \mid m)\), for \(n \le m\),
    using your answer to part (b).

    By Bayes' rule, we have
    \begin{align*}
      p_{N \mid M}(n \mid m) &= \frac{p_{N}(n) \, p_{M \mid N}(m \mid n)}{p_M(m)}.
    \end{align*}
    For the denominator, by total probability theorem we have
    \begin{align*}
      p_M(m) &= \sum_{n=0}^∞ p_{N}(n) \, p_{M \mid N}(m \mid n) \\
             &= \sum_{n=0}^m \frac{(λs)^m(t/s)^n}{n!(m-n)!} e^{-λ(t+s)} \\
             &= \frac{(λs)^m}{m!} e^{-λ(t+s)} \sum_{n=0}^m \binom{m}{n}\paren{\frac{t}{s}}^n \\
             &= \paren{1 + \frac{t}{s}}^m \frac{(λs)^m}{m!} e^{-λ(t+s)} \\
             &= \frac{[λ(t+s)]^m}{m!} e^{-λ(t+s)}.
    \end{align*}
    By looking at the above result, we recognize that \(p_M\) can also
    be derived by direct application of Poission \textsc{pmf}.

    Finally, when \(n \le m\) we have
    \begin{align*}
      p_{N \mid M}(n \mid m) &= \frac{\frac{(λs)^m(t/s)^n}{n!(m-n)!} e^{-λ(t+s)}}{\paren{1 + \frac{t}{s}}^m \frac{(λs)^m}{m!} e^{-λ(t+s)}} \\
                             &= \binom{m}{n} \paren{\frac{t}{s}}^n \paren{1 + \frac{t}{s}}^{-m}.
    \end{align*}

  \item \currentpdfbookmark{d}{5.d} Rederive your answer to part (c) without using part (b).  As a
    hint, consider what kind of distribution the \(k\)th arrival time
    has if we are given the event \(\{M = m\}\), where \(k \le m\).

    Some references for the relationship between uniform distribution
    and Poisson process can be found at
    \href{http://www.maths.qmul.ac.uk/~ig/MAS338/PP\%20and\%20uniform\%20d-n.pdf}{\em
      The Uniform Distribution and the Poisson Process} by Ilya
    Goldsheid, section
    \href{http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-PP.pdf}{1.10}
    from \emph{Notes on the Poisson Process} by Karl Sigman, theorem
    \href{https://www.math.ucdavis.edu/~gravner/MAT135B/materials/ch18.pdf}{18.5}
    from \emph{Lecture Notes for Introductory Probability} by Janko
    Gravner.

    The description of the hint is somewhat misleading.  It should be
    read as ``consider the joint distribution of \(m\) arrival times given \(M = m\).''

    Conditioned upon \(M=m\), the joint \textsc{pdf} of
    \(m\) arrival times \(X_1\), \dots, \(X_m\) is
    \begin{align*}
      f_{X_1, \dots, X_m \mid M}(x_1, \dots, x_m \mid m)
      &= \frac{f_{T_1}\!(x_1) \, f_{T_2}\!(x_2-x_1) \cdots f_{T_n}\!(x_n-x_{n-1}) \, P(T_{n+1} > t+s-x_n)}{P(M = m)} \\
      &= \frac{λe^{-λx_1} λe^{-λ(x_2-x_1)} \cdots λe^{-λ(x_n-x_{n-1})} e^{-λ(t+s-x_n)}}{P(M = m)} \\
      &= \frac{λ^m e^{-λ(t+s)}}{\frac{λ^m(t+s)^m}{m!} e^{-λ(t+s)}} = \frac{m!}{(t+s)^m}
    \end{align*}
    for \(0 < x_1 < x_2 < \dots < x_m < t+s\), where \(T_k\) is the \(k\)th
    interarrival time.  This is also the joint \textsc{pdf} of the
    order statistics of some uniform r.v.s \(U_1\), \(U_2\), \dots,
    \(U_m\).  That is
    \[f_{X_1, \dots, X_m \mid M}(x_1, \dots, x_m \mid m) = f_{U_{(1)}, \dots, U_{(m)}}(x_1, \dots, x_m).\]
    Thus,
    \begin{align*}
      p_{N \mid M}(n \mid m) &= P(X_n < t < X_{n+1}  \mid M = m) \\
                             &= P(U_{(n)} < t < U_{(n+1)}) \\
                             &= \binom{m}{n} P(U < t)^n P(U > t)^{m-n} \\
                             &= \binom{m}{n} \paren{\frac{t}{t+s}}^n \paren{\frac{s}{t+s}}^{m-n}.
    \end{align*}

  \item \currentpdfbookmark{e}{5.e} Find \(\E[NM]\).

    By the law of iterated expectation, we have
    \begin{align*}
      \E[NM] &= \E[\E[NM \mid N]] \\
             &= \E[N \E[M \mid N]] \\
             &= \E[N (N + \E[M-N \mid N])] \\
             &= \E[N (N + λs)] \\
             &= \E[N^2] + λs \E[N] \\
             &= λt (1 + λs).
    \end{align*}
  \end{enumerate}

\item \pdfbookmark{6}{6} The interarrival times for cars passing a
  checkpoint are independent random variables with \textsc{pdf}
  \[f_T(t) =
    \begin{cases}
      2e^{-2t}, & \text{for } t > 0, \\
      0, & \text{otherwise.}
    \end{cases}
  \]
  where the interarrival times are measured in minutes.  The
  successive experimental values of the durations of these
  interarrival times are recorded on small computer cards.  The
  recording operation occupies a negligible time period following each
  arrival.  Each card has space for three entries.  As soon as a card
  is filled, it is replaced by the next card.
  \begin{enumerate}
  \item \subpdfbookmark{a}{6.a} Determine the mean and the third
    moment of the interarrival times.

    We can either integrate directly or use moment generating
    function.  By direct integration, the mean is
    \begin{align*}
      \E[T] &= \int_0^{+∞} t \, λe^{-λt} \dt = \int_{+∞}^0 t \diff e^{-λt} \\
            &= te^{-λt} \Big\vert_{+∞}^0 + \frac1λ \int_{+∞}^0 \diff e^{-λt} \\
            &= \frac1λ = \frac12.
    \end{align*}
    The \(k\)th moment of the interarrival time can be found inductively.
    \begin{align*}
      \E[T^k] &= \int_0^{+∞} t^k λe^{-λt} \dt = \int_{+∞}^0 t^k \diff e^{-λt} \\
              &= t^k e^{-λt} \Big\vert_{+∞}^0 + \frac{k}{λ} \int_0^{+∞} t^{k-1} λe^{-λt} \dt \\
              &= \frac{k}{λ} \E[T^{k-1}] \\
              &= \frac{k}{λ} \frac{k-1}{λ} \dots \frac{1}{λ} \\
              &= \frac{k!}{λ^k}.
    \end{align*}
    Thus, \(\E[T^3] = 6/λ^3 = 3/4\).

    To use moment generating function, we have
    \begin{align*}
      M_s(T) &= \E[e^{sT}] = \int_0^{+∞} e^{st} λe^{-λt} \dt \\
             &= \frac{λ}{s-λ} e^{(s-λ)t} \Big\vert_0^{+∞} \\
             &= \frac{λ}{λ-s}
    \end{align*}
    for \(s < λ\).  Thus, we have
    \begin{align*}
      \E[T] &= \frac{\diff}{\diff s} M_s(T) \bigg\vert_{s=0} \\
            &= \frac{λ}{(λ-s)^2} \bigg\vert_{s=0} \\
            &= \frac1λ = \frac12, \\
      \intertext{and}
      \E[T^3]
            &= \frac{\diff^3}{\diff s^3} M_s(T) \bigg\vert_{s=0} \\
            &= \frac{6λ}{(λ-s)^4} \bigg\vert_{s=0} \\
            &= \frac{6}{λ^3} = \frac34.
    \end{align*}

  \item \currentpdfbookmark{b}{6.b} Given that no car has arrived in
    the last four minutes, determine the \textsc{pmf} for random variable \(K\),
    the number of cars to arrive in the next six minutes.

    Due to the memorylessness property of Poisson process, we have
    \[p_K(k) = \frac{12^k}{k!} e^{-12}.\]

  \item \currentpdfbookmark{c}{6.c} Determine the \textsc{pdf} and the
    expected value for the total time required to use up the first
    dozen computer cards.

    This is equivalent to ask for the \((3 \times 12)\)th arrival
    time.  Thus, the \textsc{pdf} and mean for such total time
    \(X_{36}\) are
    \begin{gather*}
      f_{X_{36}}(x) = \frac{2^{36} x^{35}}{35!} e^{-2x} \\
      \intertext{and}
      \E[X_{36}] = \frac{36}{2} = 18.
    \end{gather*}

  \item \currentpdfbookmark{d}{6.d} Consider the following two
    experiments:
    \begin{enumerate}
    \item \subpdfbookmark{i}{6.d.i} Pick a card at random from a group
      of completed cards and note the total time, \(Y\), the card was
      in service.  Find \(\E[Y]\) and \(\var(Y)\).

      It is easy to see that \(Y = X_3\).  So we have \(\E[Y] = 3/2\)
      and \(\var(Y) = 3/4\).

    \item \currentpdfbookmark{ii}{6.d.ii} Come to the corner at a random
      time.  When the card in use at the time of your arrival is
      completed, note the total time it was in service (the time from
      the start of its service to its completion).  Call this time
      \(W\).  Determine \(\E[W]\) and \(\var(W)\).

      Again, this is a random incidence problem.  Thus, the service
      time \(W\) is an Erlang distribution of order \(4\) and we have
      \(\E[W] = 4/2 = 2\) and \(\var(W) = 4/4 = 1\).
    \end{enumerate}
  \end{enumerate}

\item[G1{\footnotemark[2]}.] \footnotetext[2]{Required for 6.431;
    optional for 6.041} \pdfbookmark{G1}{G1} Consider a Poisson
  process with rate \(λ\), and let \(N(G_i)\) denote the number of
  arrivals of the process during an interval
  \(G_i = (t_i, t_i + c_i]\).  Suppose we have \(n\) such intervals,
  \(i = 1,2,··· , n\), mutually disjoint.  Denote the union of these
  intervals by \(G\), and their total length by
  \(c = c_1 + c_2 + \dots + c_n\).  Given \(k_i \ge 0\) and with
  \(k = k_1 + k_2 + \dots + k_n\), determine
  \[P(N(G_1) = k_1, N(G_2) = k_2, \dots, N(G_n) = k_n \mid N(G) = k).\]

  Since \(k = \sum_{i=1}^n k_i\) and \(G = \bigcup_{1 \le i \le n} G_i\),
  we can see that \(\bigcap_{1 \le i \le n} \{N(G_i) = k_i\}\) is a
  subset of \(\{N(G) = k\}\).  This means
  \((\bigcap_{1 \le i \le n} \{N(G_i) = k_i\}) \cap \{N(G) = k\} =
  \bigcap_{1 \le i \le n} \{N(G_i) = k_i\}\).  Thus, also due to the
  independence property of Poisson process, we have
  \begin{align*}
    P(N(G_1) = k_1, \dots, N(G_n) = k_n \mid N(G) = k)
    &= \frac{P(N(G_1) = k_1, \dots, N(G_n) = k_n, N(G) = k)}{P(N(G) = k)} \\
    &= \frac{P(N(G_1) = k_1, \dots, N(G_n) = k_n)}{P(N(G) = k)} \\
    &= \frac{P(N(G_1) = k_1) \cdots P(N(G_n) = k_n)}{P(N(G) = k)} \\
    &= \frac{\frac{(λc_1)^{k_1}}{k_1!} e^{-λc_1} \frac{(λc_2)^{k_2}}{k_2!} e^{-λc_2} \cdots \frac{(λc_n)^{k_n}}{k_n!} e^{-λc_n}}{\frac{(λc)^k}{k!} e^{-λc}} \\
    &= \frac{\frac{λ^{k_1 + k_2 + \dots + k_n} c_1^{k_1} c_2^{k_2} \cdots c_n^{k_n}}{k_1! k_2! \cdots k_n!} e^{-λ(c_1 + c_2 \dots + c_n)}}{\frac{λ^k c^k}{k!} e^{-λc}} \\
    &= \binom{k}{k_1, \dots, k_n} \paren{\frac{c_1}{c}}^{k_1} \cdots \paren{\frac{c_n}{c}}^{k_n}.
  \end{align*}
\end{enumerate}
\end{document}